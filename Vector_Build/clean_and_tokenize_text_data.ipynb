{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import string\n",
    "import pandas as pd\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenise text data\n",
    "\n",
    "A notebook to tokenize, lemmatize and clean the text data using `spacy`.\n",
    "\n",
    "Tokenized data will be saved in `Data/Tokenised_Text/`\n",
    "\n",
    "Actual process:\n",
    "1. Lemmatise\n",
    "2. Remove stopwords and punctuation\n",
    "3. Tokenise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = os.path.join(os.pardir, 'Data')\n",
    "OUT_PATH = os.path.join(os.pardir, 'Data', 'Tokenised_Text')\n",
    "TRAIN_FILE = 'train.csv'\n",
    "TEST_FILE = 'test.csv'\n",
    "\n",
    "train_path = os.path.join(INPUT_PATH, TRAIN_FILE)\n",
    "test_path = os.path.join(INPUT_PATH, TEST_FILE)\n",
    "train_out_path = os.path.join(OUT_PATH, 'tokenised_train.csv')\n",
    "test_out_path = os.path.join(OUT_PATH, 'tokenised_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define processing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify punctuation and stopwords to ignore\n",
    "punctuations = string.punctuation\n",
    "stopwords = list(STOP_WORDS)\n",
    "\n",
    "# Get a spaCy model for English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def spacy_tokenizer(sentence):\n",
    "    mytokens = parser(sentence)\n",
    "    mytokens = [word.lemma_.lower().strip() \n",
    "                if word.lemma_ != '-PRON-' else word.lower_ \n",
    "                for word in mytokens]   \n",
    "    mytokens = [word for word in mytokens \n",
    "                if word not in stopwords \n",
    "                and word not in punctuations]  \n",
    "    return ' '.join([i for i in mytokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall processing function\n",
    "def process(df_path):\n",
    "    df = pd.read_csv(df_path)\n",
    "    df.set_index('id', inplace=True)\n",
    "    df['comment_text'] = df['comment_text'].apply(spacy_tokenizer)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process\n",
    "\n",
    "**Note:** In final submission we cannot use a preprocessed test set - it must be done in the submission kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1804874, 44)\n",
      "Wall time: 23min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df = process(train_path)\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(97320, 1)\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_df = process(test_path)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(train_out_path)\n",
    "test_df.to_csv(test_out_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}